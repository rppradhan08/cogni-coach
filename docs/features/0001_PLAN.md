## Feature Plan: Resume Optimizer (0001)

### Brief description
Build a Resume Optimizer that allows users to upload or paste resume text and input a target job description, performs ATS readability checks, computes keyword matching with a relevance score and missing keyword suggestions, provides clarity/impact improvements and basic grammar/typo corrections, and returns an actionable report with an option to download an ATS-optimized version.

Verbatim requirements from the user:
- Resume Optimizer
  - Upload or paste resume text; input target job description
  - ATS readability checks (layout, fonts, parse-ability, consistency)
  - Keyword matching with relevance score and missing keyword suggestions
  - Clarity and impact improvements (e.g., stronger verbs, scannable bullets)
  - Basic grammar/typo corrections
  - Actionable report and option to download an ATS-optimized version

Stack context: Frontend React + TypeScript (Vite); Tailwind + shadcn/ui; Auth via Clerk; API via Hono; Database via Supabase (Postgres); Package pnpm; AI: OpenAI's gpt-4o-mini model for keyword extraction/refinement, clarity/impact rewrites, and grammar/typo corrections.

### Files and modules (new to create; no existing files to change)
- Frontend (Vite app)
  - `src/pages/resume-optimizer.tsx`
  - `src/components/resume/ResumeInput.tsx` (upload/paste)
  - `src/components/resume/JobDescriptionInput.tsx`
  - `src/components/resume/AnalysisRunner.tsx` (submit/loader/state)
  - `src/components/resume/AnalysisReport.tsx` (scores, issues, suggestions)
  - `src/components/resume/DownloadOptimized.tsx`
  - `src/lib/api/resume.ts` (typed API client)
  - `src/types/resume.ts` (UI types mirrored from API)
- Backend (Hono API)
  - `server/index.ts` (Hono app bootstrap)
  - `server/routes/resume.ts` (router)
  - `server/services/atsReadability.ts`
  - `server/services/keywordMatching.ts`
  - `server/services/clarityImpact.ts`
  - `server/services/grammarTypos.ts`
  - `server/services/report.ts` (compose final report + optimize text)
  - `server/clients/llm.ts` (LLM wrapper using OpenAI's gpt-4o-mini model)
  - `server/db/schema.sql` (DDL for tables)
  - `server/db/queries.ts` (persistence)

### Data model (Postgres/Supabase)
- Table `resumes`
  - id (uuid, pk), user_id (uuid), source_type (enum: pasted|pdf|docx), original_text (text), optimized_text (text, nullable), created_at (timestamptz)
- Table `resume_analyses`
  - id (uuid, pk), resume_id (uuid, fk), job_description (text), ats_score (int 0–100), keyword_relevance_score (int 0–100), missing_keywords (text[]), issues (jsonb), suggestions (jsonb), report (jsonb), created_at (timestamptz)

Notes:
- `issues` holds ATS/layout problems, parse warnings, and grammar counts.
- `suggestions` holds clarity/impact rewrites and keyword insertion candidates.
- `report` is the composed, presentation-friendly structure used by UI.

### API design (Hono)
- POST `/api/resume/analyze`
  - Input: { resumeInput: { type: pasted|pdf|docx, text?: string, fileId?: string }, jobDescription: string }
  - Auth: required (Clerk)
  - Output: { analysisId, atsScore, keywordRelevanceScore, missingKeywords[], issues[], suggestions[], report }
- POST `/api/resume/optimize`
  - Input: { analysisId }
  - Output: { optimizedResumeText, downloadToken }
- GET `/api/resume/report/:analysisId`
  - Output: { report }
- POST `/api/files`
  - Upload endpoint used when source is file; returns { fileId } (stored via Supabase storage)

### Algorithms (step-by-step)
1) ATS readability checks
  - If source is file: extract text
    - PDF: extract using a PDF text extractor; DOCX: extract using a DOCX extractor
    - Normalize whitespace, preserve bullet indicators where possible
  - Structure heuristics on normalized text
    - Section detection: scan for common headings (Experience, Education, Skills, Summary)
    - Two-column heuristic: detect frequent short lines with consistent left/right alignment markers (limited in text-only extraction); if ambiguous, warn
    - Parse-ability: flag excessive tables/graphics markers; excessive special characters; inconsistent bullet symbols
    - Consistency: detect mixed date formats; inconsistent capitalization of headings; irregular indentation for bullets
  - Font checks (best-effort): for DOCX only, flag non-standard fonts if metadata available; for plain text/PDF text, skip and document limitation
  - Produce: list of `issues` with severities and remediation hints

2) Keyword matching with relevance score and missing keyword suggestions
  - Extract candidate keywords/skills from job description
    - Tokenize; remove stopwords; noun-phrase and skill-phrase extraction (simple n-grams + hand-curated skill dictionary; optionally refined using OpenAI's gpt-4o-mini model)
  - Extract candidate keywords from resume text using same method
  - Compute weighted relevance score
    - Build sets with weights (TF-IDF-like weighting or frequency capped)
    - Score = weighted Jaccard or cosine similarity of vectorized keyword sets, scaled 0–100
  - Missing keywords: top N job-description keywords absent or below frequency threshold in resume

3) Clarity and impact improvements
  - Detect weak phrases using rule list (e.g., "responsible for", "helped with", passive voice cues)
  - Suggest stronger verbs and quantification prompts
    - For each weak bullet: generate rewrite suggestions emphasizing action + result + metric
    - Ensure bullets remain scannable (≤2 lines; start with a verb)

4) Basic grammar/typo corrections
  - Run grammar/spell analysis using OpenAI's gpt-4o-mini model (or fallback grammar service if unavailable)
  - Produce corrected variants and a diff summary of changes

5) Actionable report and ATS-optimized version
  - Compose report sections: Summary (scores), ATS issues, Missing keywords, Suggested rewrites, Grammar fixes
  - Optimization pass
    - Insert missing keywords where contextually appropriate (without stuffing)
    - Normalize formatting (single-column, standard headings, consistent date format, simple bullet style)
  - Output optimized resume as plain text or Markdown; provide download token

### UI flow (frontend)
- Page `resume-optimizer`
  - Step 1: Input panel (paste text or upload file) + Job Description textarea
  - Step 2: Run analysis (loading state), display scores and key highlights first
  - Step 3: Detailed sections (ATS issues, Keywords, Clarity/Impact, Grammar)
  - Step 4: Generate optimized version and provide Download button

### Phases
- Phase 1 – Data layer
  - Create tables `resumes`, `resume_analyses`; set up Supabase storage bucket for uploads
  - Clerk user linking; basic row-level security policies (user_id ownership)
- Phase 2 – API
  - Implement `/api/files`, `/api/resume/analyze`, `/api/resume/optimize`, `/api/resume/report/:id`
  - Integrate LLM wrapper; implement services per algorithm steps
- Phase 3 – UI
  - Build input forms, run analysis, render report, optimized download
  - Add to navigation; protect route with auth
- Phase 4 – Polish
  - Rate limiting, error handling, loading/empty/error states, analytics events

### Notes/assumptions
- File parsing quality varies; where metadata is unavailable, font checks are best-effort
- Optimized output is plain text/Markdown in MVP; DOCX export can be added later
- LLM (OpenAI's gpt-4o-mini) prompts must instruct against keyword stuffing and preserve factual accuracy


